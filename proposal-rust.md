# AI è¯¾ç¨‹åˆ†å¸ƒå¼ä»»åŠ¡æäº¤ç³»ç»Ÿè®¾è®¡æ–‡æ¡£ (Rust ç‰ˆ)

## 1. ç³»ç»Ÿæ¦‚è¿°

### 1.1 æ¶æ„æ€»è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              LOCAL (5090 Node)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  CLI    â”‚ â”€â”€â”€â–¶ â”‚         Local Proxy Server          â”‚                   â”‚
â”‚  â”‚(student)â”‚ gRPC â”‚  - User session management          â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  or  â”‚  - File cache & sync management     â”‚                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” HTTP â”‚  - Task status polling              â”‚                   â”‚
â”‚  â”‚  CLI    â”‚ â”€â”€â”€â–¶ â”‚  - Transport client                 â”‚                   â”‚
â”‚  â”‚(student)â”‚      â”‚  - Web UI for status                â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Transport Layer (trait)   â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚   S3   â”‚ â”‚   Redis    â”‚  â”‚
                    â”‚  â”‚ (MinIO)â”‚ â”‚  Pub/Sub   â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚  HTTP  â”‚ â”‚    NATS    â”‚  â”‚
                    â”‚  â”‚ Pollingâ”‚ â”‚            â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              REMOTE CLUSTER                                  â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                      â”‚    Remote Server      â”‚                              â”‚
â”‚                      â”‚  - Task queue mgmt    â”‚                              â”‚
â”‚                      â”‚  - User accounting    â”‚                              â”‚
â”‚                      â”‚  - Env reproduction   â”‚                              â”‚
â”‚                      â”‚  - Slurm submission   â”‚                              â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                  â”‚                                          â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                      â”‚    Slurm Cluster      â”‚                              â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ ¸å¿ƒè®¾è®¡åŸåˆ™

1. **Transport æŠ½è±¡**ï¼šæ§åˆ¶é¢é€šä¿¡é€šè¿‡ trait æŠ½è±¡ï¼Œæ”¯æŒå¤šç§åç«¯
2. **Data Plane ä¸ Control Plane åˆ†ç¦»**ï¼š
   - Control Plane: æ¶ˆæ¯ã€çŠ¶æ€ã€å‘½ä»¤ï¼ˆèµ° Transportï¼‰
   - Data Plane: å¤§æ–‡ä»¶ä¼ è¾“ï¼ˆå§‹ç»ˆèµ°å¯¹è±¡å­˜å‚¨ï¼‰
3. **Workspace ç»„ç»‡**ï¼šå¤š crate ç»“æ„ï¼Œå…±äº«æ ¸å¿ƒç±»å‹

## 2. Rust é¡¹ç›®ç»“æ„

### 2.1 Workspace å¸ƒå±€

```
ailab/
â”œâ”€â”€ Cargo.toml                    # Workspace root
â”œâ”€â”€ README.md
â”œâ”€â”€ config/                       # é…ç½®æ–‡ä»¶ç¤ºä¾‹
â”‚   â”œâ”€â”€ proxy.example.toml
â”‚   â””â”€â”€ remote.example.toml
â”‚
â”œâ”€â”€ crates/
â”‚   â”œâ”€â”€ ailab-core/              # æ ¸å¿ƒç±»å‹å’Œ trait å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ lib.rs
â”‚   â”‚       â”œâ”€â”€ types/           # å…±äº«æ•°æ®ç±»å‹
â”‚   â”‚       â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ task.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ user.rs
â”‚   â”‚       â”‚   â””â”€â”€ message.rs
â”‚   â”‚       â”œâ”€â”€ transport/       # Transport trait å®šä¹‰
â”‚   â”‚       â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚       â”‚   â””â”€â”€ traits.rs
â”‚   â”‚       â”œâ”€â”€ storage/         # Storage trait å®šä¹‰
â”‚   â”‚       â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚       â”‚   â””â”€â”€ traits.rs
â”‚   â”‚       â”œâ”€â”€ hash.rs          # Hash è®¡ç®—å·¥å…·
â”‚   â”‚       â””â”€â”€ error.rs         # ç»Ÿä¸€é”™è¯¯ç±»å‹
â”‚   â”‚
â”‚   â”œâ”€â”€ ailab-transport/         # Transport å®ç°
â”‚   â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ lib.rs
â”‚   â”‚       â”œâ”€â”€ s3.rs            # S3/MinIO å®ç°
â”‚   â”‚       â”œâ”€â”€ redis.rs         # Redis Pub/Sub å®ç°
â”‚   â”‚       â”œâ”€â”€ http.rs          # HTTP é•¿è½®è¯¢å®ç°
â”‚   â”‚       â””â”€â”€ nats.rs          # NATS å®ç° (å¯é€‰)
â”‚   â”‚
â”‚   â”œâ”€â”€ ailab-storage/           # å¯¹è±¡å­˜å‚¨å®ç°
â”‚   â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ lib.rs
â”‚   â”‚       â”œâ”€â”€ s3.rs            # S3 å…¼å®¹å­˜å‚¨
â”‚   â”‚       â””â”€â”€ local.rs         # æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ (æµ‹è¯•ç”¨)
â”‚   â”‚
â”‚   â”œâ”€â”€ ailab-cli/               # CLI å·¥å…·
â”‚   â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ main.rs
â”‚   â”‚       â”œâ”€â”€ commands/
â”‚   â”‚       â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ auth.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ task.rs
â”‚   â”‚       â”‚   â””â”€â”€ data.rs
â”‚   â”‚       â””â”€â”€ config.rs
â”‚   â”‚
â”‚   â”œâ”€â”€ ailab-proxy/             # Local Proxy Server
â”‚   â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ main.rs
â”‚   â”‚       â”œâ”€â”€ server.rs        # Axum HTTP server
â”‚   â”‚       â”œâ”€â”€ api/
â”‚   â”‚       â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ auth.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ tasks.rs
â”‚   â”‚       â”‚   â””â”€â”€ data.rs
â”‚   â”‚       â”œâ”€â”€ services/
â”‚   â”‚       â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ user.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ sync.rs
â”‚   â”‚       â”‚   â”œâ”€â”€ cache.rs
â”‚   â”‚       â”‚   â””â”€â”€ poller.rs
â”‚   â”‚       â”œâ”€â”€ db/
â”‚   â”‚       â”‚   â””â”€â”€ mod.rs       # SQLite via sqlx
â”‚   â”‚       â””â”€â”€ web/
â”‚   â”‚           â”œâ”€â”€ mod.rs
â”‚   â”‚           â”œâ”€â”€ templates/   # Askama æ¨¡æ¿
â”‚   â”‚           â””â”€â”€ handlers.rs
â”‚   â”‚
â”‚   â””â”€â”€ ailab-remote/            # Remote Server
â”‚       â”œâ”€â”€ Cargo.toml
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ main.rs
â”‚           â”œâ”€â”€ server.rs
â”‚           â”œâ”€â”€ services/
â”‚           â”‚   â”œâ”€â”€ mod.rs
â”‚           â”‚   â”œâ”€â”€ processor.rs
â”‚           â”‚   â”œâ”€â”€ env_builder.rs
â”‚           â”‚   â”œâ”€â”€ slurm.rs
â”‚           â”‚   â”œâ”€â”€ monitor.rs
â”‚           â”‚   â””â”€â”€ accounting.rs
â”‚           â””â”€â”€ db/
â”‚               â””â”€â”€ mod.rs
â”‚
â””â”€â”€ tests/                       # é›†æˆæµ‹è¯•
    â””â”€â”€ integration/
```

### 2.2 Workspace Cargo.toml

```toml
[workspace]
resolver = "2"
members = [
    "crates/ailab-core",
    "crates/ailab-transport",
    "crates/ailab-storage",
    "crates/ailab-cli",
    "crates/ailab-proxy",
    "crates/ailab-remote",
]

[workspace.package]
version = "0.1.0"
edition = "2021"
license = "MIT"
repository = "https://github.com/yourorg/ailab"

[workspace.dependencies]
# Async runtime
tokio = { version = "1", features = ["full"] }

# Serialization
serde = { version = "1", features = ["derive"] }
serde_json = "1"
toml = "0.8"

# HTTP & API
axum = { version = "0.7", features = ["macros"] }
tower = "0.4"
tower-http = { version = "0.5", features = ["cors", "trace"] }
reqwest = { version = "0.12", features = ["json", "rustls-tls"], default-features = false }

# CLI
clap = { version = "4", features = ["derive"] }

# Database
sqlx = { version = "0.8", features = ["runtime-tokio", "sqlite"] }

# S3
aws-sdk-s3 = "1"
aws-config = "1"

# Redis
redis = { version = "0.25", features = ["tokio-comp", "connection-manager"] }

# Logging & Tracing
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# Utils
uuid = { version = "1", features = ["v4", "serde"] }
chrono = { version = "0.4", features = ["serde"] }
sha2 = "0.10"
hex = "0.4"
thiserror = "1"
anyhow = "1"
async-trait = "0.1"
futures = "0.3"
walkdir = "2"
ignore = "0.4"  # For .gitignore-style patterns
tar = "0.4"
flate2 = "1"
directories = "5"  # For XDG paths
indicatif = "0.17"  # Progress bars

# Template
askama = "0.12"
askama_axum = "0.4"
```

## 3. æ ¸å¿ƒç±»å‹å®šä¹‰ (ailab-core)

### 3.1 Transport Trait

```rust
// crates/ailab-core/src/transport/traits.rs

use async_trait::async_trait;
use crate::types::message::{ControlMessage, MessageId};
use crate::error::Result;

/// æ§åˆ¶é¢æ¶ˆæ¯ä¼ è¾“çš„æ–¹å‘
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum Channel {
    /// Local Proxy -> Remote Server
    ToRemote,
    /// Remote Server -> Local Proxy  
    ToLocal,
}

/// æ§åˆ¶é¢ä¼ è¾“å±‚æŠ½è±¡
/// 
/// ç”¨äºä¼ è¾“å°å‹æ§åˆ¶æ¶ˆæ¯ï¼ˆä»»åŠ¡æäº¤ã€çŠ¶æ€æ›´æ–°ç­‰ï¼‰
/// å¤§æ–‡ä»¶ä¼ è¾“åº”ä½¿ç”¨ Storage trait
#[async_trait]
pub trait Transport: Send + Sync + 'static {
    /// å‘é€æ¶ˆæ¯åˆ°æŒ‡å®šé€šé“
    async fn send(&self, channel: Channel, message: ControlMessage) -> Result<MessageId>;
    
    /// ä»æŒ‡å®šé€šé“æ¥æ”¶æ¶ˆæ¯ï¼ˆé˜»å¡ç›´åˆ°æœ‰æ¶ˆæ¯æˆ–è¶…æ—¶ï¼‰
    /// è¿”å› None è¡¨ç¤ºè¶…æ—¶æ— æ¶ˆæ¯
    async fn receive(&self, channel: Channel, timeout: std::time::Duration) -> Result<Option<ControlMessage>>;
    
    /// ç¡®è®¤æ¶ˆæ¯å·²å¤„ç†ï¼ˆç”¨äºå®ç° at-least-once è¯­ä¹‰ï¼‰
    async fn ack(&self, channel: Channel, message_id: &MessageId) -> Result<()>;
    
    /// è·å–æœªç¡®è®¤çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆç”¨äºæ¢å¤ï¼‰
    async fn list_pending(&self, channel: Channel) -> Result<Vec<ControlMessage>>;
}

/// Transport å·¥å‚ï¼Œç”¨äºæ ¹æ®é…ç½®åˆ›å»ºå®ä¾‹
pub trait TransportFactory {
    fn create(config: &TransportConfig) -> Result<Box<dyn Transport>>;
}

#[derive(Debug, Clone, serde::Deserialize)]
#[serde(tag = "type", rename_all = "lowercase")]
pub enum TransportConfig {
    S3 {
        endpoint: String,
        bucket: String,
        access_key: String,
        secret_key: String,
        region: Option<String>,
        prefix: Option<String>,
        poll_interval_ms: Option<u64>,
    },
    Redis {
        url: String,
        prefix: Option<String>,
    },
    Http {
        /// å¯¹æ–¹çš„ HTTP ç«¯ç‚¹ï¼ˆéœ€è¦æœ‰ä¸€ç«¯æœ‰å…¬ç½‘ï¼‰
        endpoint: String,
        /// è®¤è¯ token
        token: Option<String>,
    },
    Nats {
        url: String,
        subject_prefix: Option<String>,
    },
}
```

### 3.2 Storage Trait

```rust
// crates/ailab-core/src/storage/traits.rs

use async_trait::async_trait;
use std::path::Path;
use tokio::io::{AsyncRead, AsyncWrite};
use crate::error::Result;

/// å¯¹è±¡å­˜å‚¨æŠ½è±¡ï¼ˆç”¨äºå¤§æ–‡ä»¶ä¼ è¾“ï¼‰
#[async_trait]
pub trait Storage: Send + Sync + 'static {
    /// ä¸Šä¼ æ–‡ä»¶
    async fn upload(&self, key: &str, path: &Path) -> Result<()>;
    
    /// ä¸Šä¼ ç›®å½•ï¼ˆæ‰“åŒ…ä¸º tar.gzï¼‰
    async fn upload_dir(&self, key: &str, path: &Path, ignore_patterns: &[String]) -> Result<()>;
    
    /// ä¸‹è½½æ–‡ä»¶
    async fn download(&self, key: &str, path: &Path) -> Result<()>;
    
    /// ä¸‹è½½å¹¶è§£å‹ç›®å½•
    async fn download_dir(&self, key: &str, path: &Path) -> Result<()>;
    
    /// æ£€æŸ¥å¯¹è±¡æ˜¯å¦å­˜åœ¨
    async fn exists(&self, key: &str) -> Result<bool>;
    
    /// åˆ é™¤å¯¹è±¡
    async fn delete(&self, key: &str) -> Result<()>;
    
    /// åˆ—å‡ºæŒ‡å®šå‰ç¼€çš„å¯¹è±¡
    async fn list(&self, prefix: &str) -> Result<Vec<String>>;
    
    /// è·å–å¯¹è±¡å¤§å°
    async fn size(&self, key: &str) -> Result<u64>;
    
    /// è·å–æµå¼è¯»å–å™¨ï¼ˆç”¨äºå¤§æ–‡ä»¶ï¼‰
    async fn get_reader(&self, key: &str) -> Result<Box<dyn AsyncRead + Unpin + Send>>;
    
    /// è·å–æµå¼å†™å…¥å™¨
    async fn get_writer(&self, key: &str) -> Result<Box<dyn AsyncWrite + Unpin + Send>>;
}

#[derive(Debug, Clone, serde::Deserialize)]
#[serde(tag = "type", rename_all = "lowercase")]
pub enum StorageConfig {
    S3 {
        endpoint: String,
        bucket: String,
        access_key: String,
        secret_key: String,
        region: Option<String>,
    },
    Local {
        root: String,
    },
}
```

### 3.3 æ ¸å¿ƒæ•°æ®ç±»å‹

```rust
// crates/ailab-core/src/types/task.rs

use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use uuid::Uuid;

pub type TaskId = Uuid;
pub type UserId = String;
pub type ContentHash = String;  // "sha256:xxxx"

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TaskConfig {
    pub name: String,
    pub description: Option<String>,
    pub resources: ResourceRequest,
    pub environment: EnvironmentConfig,
    pub files: FileMapping,
    pub run: RunConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ResourceRequest {
    pub gpus: u32,
    pub cpus: u32,
    pub memory: String,         // "32G"
    pub time_limit: String,     // "4:00:00"
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EnvironmentConfig {
    pub env_name: String,
    #[serde(default)]
    pub extra_wheels: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileMapping {
    #[serde(default)]
    pub upload: Vec<PathMapping>,
    #[serde(default)]
    pub download: Vec<PathMapping>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PathMapping {
    pub local: String,
    pub remote: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RunConfig {
    #[serde(default = "default_workdir")]
    pub workdir: String,
    pub command: Option<String>,
    pub script: Option<String>,
}

fn default_workdir() -> String {
    ".".to_string()
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum TaskStatus {
    /// æ­£åœ¨ä¸Šä¼ æ–‡ä»¶åˆ°å­˜å‚¨
    Uploading,
    /// å·²ä¸Šä¼ ï¼Œç­‰å¾…è¿œç«¯å¤„ç†
    Pending,
    /// è¿œç«¯æ­£åœ¨å‡†å¤‡ç¯å¢ƒ
    Preparing,
    /// å·²æäº¤åˆ° Slurmï¼Œæ’é˜Ÿä¸­
    Queued,
    /// Slurm ä»»åŠ¡æ­£åœ¨è¿è¡Œ
    Running,
    /// ä»»åŠ¡å®Œæˆï¼Œç»“æœå¾…ä¸‹è½½
    Completed,
    /// ä»»åŠ¡å¤±è´¥
    Failed,
    /// ä»»åŠ¡è¢«å–æ¶ˆ
    Cancelled,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Task {
    pub id: TaskId,
    pub user_id: UserId,
    pub config: TaskConfig,
    pub status: TaskStatus,
    
    // Hashes for caching
    pub env_hash: ContentHash,
    pub project_hash: ContentHash,
    pub data_hashes: Vec<ContentHash>,
    pub whl_hashes: Vec<ContentHash>,
    
    // Timestamps
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub started_at: Option<DateTime<Utc>>,
    pub completed_at: Option<DateTime<Utc>>,
    
    // Remote info
    pub slurm_job_id: Option<String>,
    pub exit_code: Option<i32>,
    pub error_message: Option<String>,
    
    // Resource usage
    pub gpu_seconds: Option<u64>,
    pub cpu_seconds: Option<u64>,
}
```

### 3.4 æ§åˆ¶æ¶ˆæ¯ç±»å‹

```rust
// crates/ailab-core/src/types/message.rs

use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use uuid::Uuid;
use super::task::{Task, TaskId, TaskStatus, ContentHash, UserId};

pub type MessageId = Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ControlMessage {
    pub id: MessageId,
    pub timestamp: DateTime<Utc>,
    pub payload: MessagePayload,
}

impl ControlMessage {
    pub fn new(payload: MessagePayload) -> Self {
        Self {
            id: Uuid::new_v4(),
            timestamp: Utc::now(),
            payload,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type", rename_all = "snake_case")]
pub enum MessagePayload {
    // ===== Local -> Remote =====
    
    /// æäº¤æ–°ä»»åŠ¡
    SubmitTask {
        task: Task,
        /// å­˜å‚¨ä¸­çš„è·¯å¾„æ˜ å°„
        storage_keys: StorageKeys,
    },
    
    /// å–æ¶ˆä»»åŠ¡
    CancelTask {
        task_id: TaskId,
        user_id: UserId,
    },
    
    /// æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ï¼ˆç”¨äºåŒæ­¥ï¼‰
    QueryStatus {
        task_ids: Vec<TaskId>,
    },
    
    // ===== Remote -> Local =====
    
    /// ä»»åŠ¡çŠ¶æ€æ›´æ–°
    StatusUpdate {
        task_id: TaskId,
        status: TaskStatus,
        slurm_job_id: Option<String>,
        progress: Option<String>,
        error_message: Option<String>,
    },
    
    /// ä»»åŠ¡å®Œæˆé€šçŸ¥
    TaskCompleted {
        task_id: TaskId,
        status: TaskStatus,  // Completed | Failed | Cancelled
        exit_code: Option<i32>,
        result_keys: Vec<String>,  // ç»“æœæ–‡ä»¶åœ¨å­˜å‚¨ä¸­çš„ key
        gpu_seconds: u64,
        cpu_seconds: u64,
        error_message: Option<String>,
    },
    
    /// æ‰¹é‡çŠ¶æ€å“åº”
    StatusResponse {
        tasks: Vec<TaskStatusInfo>,
    },
    
    // ===== Bidirectional =====
    
    /// å¿ƒè·³/å¥åº·æ£€æŸ¥
    Heartbeat {
        source: String,  // "proxy" | "remote"
        timestamp: DateTime<Utc>,
    },
    
    /// ACK ç¡®è®¤
    Ack {
        message_id: MessageId,
    },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StorageKeys {
    pub env: String,           // envs/{env_hash}.tar.gz
    pub project: String,       // projects/{project_hash}.tar.gz
    pub data: Vec<String>,     // datasets/{hash}/
    pub whls: Vec<String>,     // whls/{hash}.whl
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TaskStatusInfo {
    pub task_id: TaskId,
    pub status: TaskStatus,
    pub slurm_job_id: Option<String>,
    pub progress: Option<String>,
    pub gpu_seconds: Option<u64>,
}
```

### 3.5 Hash è®¡ç®—

```rust
// crates/ailab-core/src/hash.rs

use sha2::{Sha256, Digest};
use std::path::Path;
use walkdir::WalkDir;
use ignore::gitignore::GitignoreBuilder;
use crate::error::Result;

const HASH_PREFIX: &str = "sha256";
const HASH_LENGTH: usize = 16;  // ä½¿ç”¨å‰16ä¸ªå­—ç¬¦

/// è®¡ç®—æ–‡ä»¶çš„ hash
pub fn hash_file(path: &Path) -> Result<String> {
    let content = std::fs::read(path)?;
    Ok(hash_bytes(&content))
}

/// è®¡ç®—å­—èŠ‚çš„ hash
pub fn hash_bytes(data: &[u8]) -> String {
    let mut hasher = Sha256::new();
    hasher.update(data);
    let result = hasher.finalize();
    format!("{}:{}", HASH_PREFIX, hex::encode(&result[..HASH_LENGTH/2]))
}

/// è®¡ç®—ç›®å½•çš„ hashï¼ˆè€ƒè™‘ ignore æ¨¡å¼ï¼‰
pub fn hash_directory(path: &Path, ignore_patterns: &[String]) -> Result<String> {
    let mut hasher = Sha256::new();
    
    // æ„å»º ignore åŒ¹é…å™¨
    let mut builder = GitignoreBuilder::new(path);
    for pattern in ignore_patterns {
        builder.add_line(None, pattern)?;
    }
    let ignore = builder.build()?;
    
    // éå†ç›®å½•ï¼ŒæŒ‰è·¯å¾„æ’åºä¿è¯ä¸€è‡´æ€§
    let mut entries: Vec<_> = WalkDir::new(path)
        .into_iter()
        .filter_map(|e| e.ok())
        .filter(|e| e.file_type().is_file())
        .filter(|e| {
            ignore.matched(e.path(), false).is_none()
        })
        .collect();
    
    entries.sort_by_key(|e| e.path().to_path_buf());
    
    for entry in entries {
        let rel_path = entry.path().strip_prefix(path)?;
        // åŒ…å«ç›¸å¯¹è·¯å¾„
        hasher.update(rel_path.to_string_lossy().as_bytes());
        // åŒ…å«æ–‡ä»¶å†…å®¹
        hasher.update(&std::fs::read(entry.path())?);
    }
    
    let result = hasher.finalize();
    Ok(format!("{}:{}", HASH_PREFIX, hex::encode(&result[..HASH_LENGTH/2])))
}

/// è®¡ç®—ç¯å¢ƒ hash (uv.lock + pyproject.toml)
pub fn hash_environment(project_dir: &Path) -> Result<String> {
    let mut hasher = Sha256::new();
    
    let uv_lock = project_dir.join("uv.lock");
    let pyproject = project_dir.join("pyproject.toml");
    
    if uv_lock.exists() {
        hasher.update(b"uv.lock:");
        hasher.update(&std::fs::read(&uv_lock)?);
    }
    
    if pyproject.exists() {
        hasher.update(b"pyproject.toml:");
        hasher.update(&std::fs::read(&pyproject)?);
    }
    
    let result = hasher.finalize();
    Ok(format!("{}:{}", HASH_PREFIX, hex::encode(&result[..HASH_LENGTH/2])))
}
```

## 4. Transport å®ç° (ailab-transport)

### 4.1 S3 Transport

```rust
// crates/ailab-transport/src/s3.rs

use ailab_core::{
    transport::{Transport, Channel, TransportConfig},
    types::message::{ControlMessage, MessageId},
    error::Result,
};
use async_trait::async_trait;
use aws_sdk_s3::Client;
use std::time::Duration;
use tokio::time::sleep;

pub struct S3Transport {
    client: Client,
    bucket: String,
    prefix: String,
    poll_interval: Duration,
}

impl S3Transport {
    pub async fn new(config: &TransportConfig) -> Result<Self> {
        let TransportConfig::S3 {
            endpoint,
            bucket,
            access_key,
            secret_key,
            region,
            prefix,
            poll_interval_ms,
        } = config else {
            anyhow::bail!("Expected S3 config");
        };
        
        let sdk_config = aws_config::from_env()
            .endpoint_url(endpoint)
            .credentials_provider(aws_sdk_s3::config::Credentials::new(
                access_key,
                secret_key,
                None,
                None,
                "static",
            ))
            .region(aws_sdk_s3::config::Region::new(
                region.clone().unwrap_or_else(|| "us-east-1".to_string())
            ))
            .load()
            .await;
        
        let client = Client::new(&sdk_config);
        
        Ok(Self {
            client,
            bucket: bucket.clone(),
            prefix: prefix.clone().unwrap_or_else(|| "messages".to_string()),
            poll_interval: Duration::from_millis(poll_interval_ms.unwrap_or(1000)),
        })
    }
    
    fn channel_prefix(&self, channel: Channel) -> String {
        let dir = match channel {
            Channel::ToRemote => "to_remote",
            Channel::ToLocal => "to_local",
        };
        format!("{}/{}", self.prefix, dir)
    }
    
    fn message_key(&self, channel: Channel, id: &MessageId) -> String {
        format!("{}/{}.json", self.channel_prefix(channel), id)
    }
    
    fn processed_key(&self, channel: Channel, id: &MessageId) -> String {
        format!("{}/processed/{}.json", self.channel_prefix(channel), id)
    }
}

#[async_trait]
impl Transport for S3Transport {
    async fn send(&self, channel: Channel, message: ControlMessage) -> Result<MessageId> {
        let key = self.message_key(channel, &message.id);
        let body = serde_json::to_vec(&message)?;
        
        self.client
            .put_object()
            .bucket(&self.bucket)
            .key(&key)
            .body(body.into())
            .content_type("application/json")
            .send()
            .await?;
        
        Ok(message.id)
    }
    
    async fn receive(&self, channel: Channel, timeout: Duration) -> Result<Option<ControlMessage>> {
        let prefix = self.channel_prefix(channel);
        let deadline = tokio::time::Instant::now() + timeout;
        
        loop {
            // åˆ—å‡ºæ¶ˆæ¯
            let response = self.client
                .list_objects_v2()
                .bucket(&self.bucket)
                .prefix(&prefix)
                .max_keys(10)
                .send()
                .await?;
            
            if let Some(contents) = response.contents {
                for object in contents {
                    let key = object.key.unwrap_or_default();
                    // è·³è¿‡ processed ç›®å½•
                    if key.contains("/processed/") {
                        continue;
                    }
                    
                    // è·å–æ¶ˆæ¯å†…å®¹
                    let get_response = self.client
                        .get_object()
                        .bucket(&self.bucket)
                        .key(&key)
                        .send()
                        .await?;
                    
                    let body = get_response.body.collect().await?.into_bytes();
                    let message: ControlMessage = serde_json::from_slice(&body)?;
                    
                    return Ok(Some(message));
                }
            }
            
            // æ£€æŸ¥è¶…æ—¶
            if tokio::time::Instant::now() >= deadline {
                return Ok(None);
            }
            
            sleep(self.poll_interval).await;
        }
    }
    
    async fn ack(&self, channel: Channel, message_id: &MessageId) -> Result<()> {
        let src_key = self.message_key(channel, message_id);
        let dst_key = self.processed_key(channel, message_id);
        
        // ç§»åŠ¨åˆ° processed ç›®å½•
        self.client
            .copy_object()
            .bucket(&self.bucket)
            .copy_source(format!("{}/{}", self.bucket, src_key))
            .key(&dst_key)
            .send()
            .await?;
        
        self.client
            .delete_object()
            .bucket(&self.bucket)
            .key(&src_key)
            .send()
            .await?;
        
        Ok(())
    }
    
    async fn list_pending(&self, channel: Channel) -> Result<Vec<ControlMessage>> {
        let prefix = format!("{}/", self.channel_prefix(channel));
        let mut messages = Vec::new();
        
        let response = self.client
            .list_objects_v2()
            .bucket(&self.bucket)
            .prefix(&prefix)
            .send()
            .await?;
        
        if let Some(contents) = response.contents {
            for object in contents {
                let key = object.key.unwrap_or_default();
                if key.contains("/processed/") {
                    continue;
                }
                
                let get_response = self.client
                    .get_object()
                    .bucket(&self.bucket)
                    .key(&key)
                    .send()
                    .await?;
                
                let body = get_response.body.collect().await?.into_bytes();
                let message: ControlMessage = serde_json::from_slice(&body)?;
                messages.push(message);
            }
        }
        
        Ok(messages)
    }
}
```

### 4.2 Redis Transport

```rust
// crates/ailab-transport/src/redis.rs

use ailab_core::{
    transport::{Transport, Channel, TransportConfig},
    types::message::{ControlMessage, MessageId},
    error::Result,
};
use async_trait::async_trait;
use redis::AsyncCommands;
use std::time::Duration;

pub struct RedisTransport {
    client: redis::Client,
    prefix: String,
}

impl RedisTransport {
    pub async fn new(config: &TransportConfig) -> Result<Self> {
        let TransportConfig::Redis { url, prefix } = config else {
            anyhow::bail!("Expected Redis config");
        };
        
        let client = redis::Client::open(url.as_str())?;
        
        Ok(Self {
            client,
            prefix: prefix.clone().unwrap_or_else(|| "ailab".to_string()),
        })
    }
    
    fn queue_key(&self, channel: Channel) -> String {
        let dir = match channel {
            Channel::ToRemote => "to_remote",
            Channel::ToLocal => "to_local",
        };
        format!("{}:queue:{}", self.prefix, dir)
    }
    
    fn processing_key(&self, channel: Channel) -> String {
        let dir = match channel {
            Channel::ToRemote => "to_remote",
            Channel::ToLocal => "to_local",
        };
        format!("{}:processing:{}", self.prefix, dir)
    }
}

#[async_trait]
impl Transport for RedisTransport {
    async fn send(&self, channel: Channel, message: ControlMessage) -> Result<MessageId> {
        let mut conn = self.client.get_multiplexed_async_connection().await?;
        let queue_key = self.queue_key(channel);
        let data = serde_json::to_string(&message)?;
        
        conn.rpush::<_, _, ()>(&queue_key, &data).await?;
        
        Ok(message.id)
    }
    
    async fn receive(&self, channel: Channel, timeout: Duration) -> Result<Option<ControlMessage>> {
        let mut conn = self.client.get_multiplexed_async_connection().await?;
        let queue_key = self.queue_key(channel);
        let processing_key = self.processing_key(channel);
        
        // BLMOVE: åŸå­åœ°ä»é˜Ÿåˆ—ç§»åŠ¨åˆ°å¤„ç†ä¸­åˆ—è¡¨
        let result: Option<String> = redis::cmd("BLMOVE")
            .arg(&queue_key)
            .arg(&processing_key)
            .arg("LEFT")
            .arg("RIGHT")
            .arg(timeout.as_secs_f64())
            .query_async(&mut conn)
            .await?;
        
        match result {
            Some(data) => {
                let message: ControlMessage = serde_json::from_str(&data)?;
                Ok(Some(message))
            }
            None => Ok(None),
        }
    }
    
    async fn ack(&self, channel: Channel, message_id: &MessageId) -> Result<()> {
        let mut conn = self.client.get_multiplexed_async_connection().await?;
        let processing_key = self.processing_key(channel);
        
        // ä» processing åˆ—è¡¨ä¸­åˆ é™¤å·²ç¡®è®¤çš„æ¶ˆæ¯
        // éœ€è¦éå†æ‰¾åˆ°å¯¹åº”çš„æ¶ˆæ¯
        let items: Vec<String> = conn.lrange(&processing_key, 0, -1).await?;
        
        for item in items {
            let msg: ControlMessage = serde_json::from_str(&item)?;
            if msg.id == *message_id {
                conn.lrem::<_, _, ()>(&processing_key, 1, &item).await?;
                break;
            }
        }
        
        Ok(())
    }
    
    async fn list_pending(&self, channel: Channel) -> Result<Vec<ControlMessage>> {
        let mut conn = self.client.get_multiplexed_async_connection().await?;
        let queue_key = self.queue_key(channel);
        
        let items: Vec<String> = conn.lrange(&queue_key, 0, -1).await?;
        
        items
            .into_iter()
            .map(|s| Ok(serde_json::from_str(&s)?))
            .collect()
    }
}
```

### 4.3 Transport å·¥å‚

```rust
// crates/ailab-transport/src/lib.rs

mod s3;
mod redis;
mod http;

pub use s3::S3Transport;
pub use redis::RedisTransport;
pub use http::HttpTransport;

use ailab_core::{
    transport::{Transport, TransportConfig},
    error::Result,
};

pub async fn create_transport(config: &TransportConfig) -> Result<Box<dyn Transport>> {
    match config {
        TransportConfig::S3 { .. } => {
            Ok(Box::new(S3Transport::new(config).await?))
        }
        TransportConfig::Redis { .. } => {
            Ok(Box::new(RedisTransport::new(config).await?))
        }
        TransportConfig::Http { .. } => {
            Ok(Box::new(HttpTransport::new(config).await?))
        }
        TransportConfig::Nats { .. } => {
            anyhow::bail!("NATS transport not implemented yet")
        }
    }
}
```

## 5. CLI è®¾è®¡ (ailab-cli)

### 5.1 å‘½ä»¤ç»“æ„

```rust
// crates/ailab-cli/src/main.rs

use clap::{Parser, Subcommand};

#[derive(Parser)]
#[command(name = "ailab")]
#[command(about = "AI Lab task submission CLI")]
#[command(version)]
struct Cli {
    /// Proxy server address
    #[arg(long, env = "AILAB_PROXY_URL", default_value = "http://localhost:8800")]
    proxy_url: String,
    
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// User authentication
    #[command(subcommand)]
    Auth(AuthCommands),
    
    /// Task management
    #[command(subcommand)]  
    Task(TaskCommands),
    
    /// Data management
    #[command(subcommand)]
    Data(DataCommands),
    
    /// Submit a task (shorthand for `task submit`)
    Submit {
        /// Path to task.toml
        #[arg(short, long, default_value = "task.toml")]
        config: String,
    },
    
    /// Show task status (shorthand for `task status`)
    Status {
        /// Task ID (optional, shows all if not provided)
        task_id: Option<String>,
    },
}

#[derive(Subcommand)]
enum AuthCommands {
    /// Register a new user
    Register {
        /// Username
        username: String,
    },
    /// Login
    Login {
        /// Username
        username: String,
    },
    /// Show current user
    Whoami,
    /// Logout
    Logout,
}

#[derive(Subcommand)]
enum TaskCommands {
    /// Submit a new task
    Submit {
        #[arg(short, long, default_value = "task.toml")]
        config: String,
    },
    /// Show task status
    Status {
        task_id: Option<String>,
    },
    /// List all tasks
    List {
        #[arg(short, long, default_value = "10")]
        limit: usize,
    },
    /// Show task logs
    Logs {
        task_id: String,
        #[arg(short, long)]
        follow: bool,
    },
    /// Cancel a task
    Cancel {
        task_id: String,
    },
}

#[derive(Subcommand)]
enum DataCommands {
    /// Pull task results
    Pull {
        task_id: String,
        #[arg(short, long)]
        output: Option<String>,
    },
    /// Pre-sync dataset to remote cache
    SyncDataset {
        path: String,
        #[arg(short, long)]
        name: Option<String>,
    },
}

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let cli = Cli::parse();
    
    // åˆå§‹åŒ– tracing
    tracing_subscriber::fmt::init();
    
    // åˆ›å»º HTTP client
    let client = reqwest::Client::new();
    
    match cli.command {
        Commands::Auth(cmd) => commands::auth::handle(cmd, &client, &cli.proxy_url).await,
        Commands::Task(cmd) => commands::task::handle(cmd, &client, &cli.proxy_url).await,
        Commands::Data(cmd) => commands::data::handle(cmd, &client, &cli.proxy_url).await,
        Commands::Submit { config } => {
            commands::task::handle(
                TaskCommands::Submit { config },
                &client,
                &cli.proxy_url,
            ).await
        }
        Commands::Status { task_id } => {
            commands::task::handle(
                TaskCommands::Status { task_id },
                &client,
                &cli.proxy_url,
            ).await
        }
    }
}
```

### 5.2 Submit å‘½ä»¤å®ç°

```rust
// crates/ailab-cli/src/commands/task.rs

use std::path::Path;
use indicatif::{ProgressBar, ProgressStyle};
use ailab_core::types::task::TaskConfig;

pub async fn submit(
    client: &reqwest::Client,
    proxy_url: &str,
    config_path: &str,
) -> anyhow::Result<()> {
    // è¯»å–é…ç½®
    let config_content = std::fs::read_to_string(config_path)?;
    let task_config: TaskConfig = toml::from_str(&config_content)?;
    
    println!("ğŸ“¦ Preparing task: {}", task_config.name);
    
    // è·å–é¡¹ç›®ç›®å½•
    let project_dir = Path::new(config_path).parent().unwrap_or(Path::new("."));
    
    // æ”¶é›†æ–‡ä»¶ä¿¡æ¯
    let pb = ProgressBar::new_spinner();
    pb.set_style(ProgressStyle::default_spinner()
        .template("{spinner:.green} {msg}")?);
    pb.set_message("Collecting files...");
    
    // è¯»å– .ailabignore
    let ignore_path = project_dir.join(".ailabignore");
    let ignore_patterns: Vec<String> = if ignore_path.exists() {
        std::fs::read_to_string(&ignore_path)?
            .lines()
            .map(String::from)
            .collect()
    } else {
        vec![]
    };
    
    // è®¡ç®— hash
    let env_hash = ailab_core::hash::hash_environment(project_dir)?;
    let project_hash = ailab_core::hash::hash_directory(project_dir, &ignore_patterns)?;
    
    pb.finish_with_message("Files collected");
    
    // æäº¤åˆ° proxy
    #[derive(serde::Serialize)]
    struct SubmitRequest {
        config: TaskConfig,
        project_dir: String,
        env_hash: String,
        project_hash: String,
        ignore_patterns: Vec<String>,
    }
    
    let request = SubmitRequest {
        config: task_config.clone(),
        project_dir: project_dir.to_string_lossy().to_string(),
        env_hash,
        project_hash,
        ignore_patterns,
    };
    
    println!("ğŸš€ Submitting to proxy...");
    
    let response = client
        .post(format!("{}/api/tasks/submit", proxy_url))
        .json(&request)
        .send()
        .await?;
    
    if response.status().is_success() {
        #[derive(serde::Deserialize)]
        struct SubmitResponse {
            task_id: String,
            message: String,
        }
        
        let result: SubmitResponse = response.json().await?;
        println!("âœ… Task submitted successfully!");
        println!("   Task ID: {}", result.task_id);
        println!("   {}", result.message);
        println!();
        println!("   Check status: ailab status {}", result.task_id);
    } else {
        let error: serde_json::Value = response.json().await?;
        println!("âŒ Submission failed: {}", error);
    }
    
    Ok(())
}
```

## 6. Local Proxy Server (ailab-proxy)

### 6.1 ä¸»æœåŠ¡å™¨

```rust
// crates/ailab-proxy/src/server.rs

use axum::{
    routing::{get, post},
    Router,
    Extension,
};
use std::sync::Arc;
use tower_http::trace::TraceLayer;

use crate::{
    api,
    services::{UserService, SyncService, CacheService, PollerService},
    db::Database,
    web,
};
use ailab_core::transport::Transport;
use ailab_core::storage::Storage;

pub struct AppState {
    pub db: Database,
    pub transport: Arc<dyn Transport>,
    pub storage: Arc<dyn Storage>,
    pub user_service: UserService,
    pub sync_service: SyncService,
    pub cache_service: CacheService,
}

pub async fn create_app(
    db: Database,
    transport: Arc<dyn Transport>,
    storage: Arc<dyn Storage>,
) -> Router {
    let user_service = UserService::new(db.clone());
    let cache_service = CacheService::new(db.clone(), storage.clone());
    let sync_service = SyncService::new(storage.clone(), cache_service.clone());
    
    let state = Arc::new(AppState {
        db,
        transport,
        storage,
        user_service,
        sync_service,
        cache_service,
    });
    
    // API routes
    let api_routes = Router::new()
        .route("/auth/register", post(api::auth::register))
        .route("/auth/login", post(api::auth::login))
        .route("/auth/whoami", get(api::auth::whoami))
        .route("/tasks/submit", post(api::tasks::submit))
        .route("/tasks", get(api::tasks::list))
        .route("/tasks/:id", get(api::tasks::get))
        .route("/tasks/:id/cancel", post(api::tasks::cancel))
        .route("/tasks/:id/logs", get(api::tasks::logs))
        .route("/data/pull/:task_id", get(api::data::pull))
        .route("/data/sync-dataset", post(api::data::sync_dataset));
    
    // Web UI routes
    let web_routes = Router::new()
        .route("/", get(web::handlers::dashboard))
        .route("/tasks", get(web::handlers::task_list))
        .route("/tasks/:id", get(web::handlers::task_detail))
        .route("/admin", get(web::handlers::admin_dashboard));
    
    Router::new()
        .nest("/api", api_routes)
        .nest("/web", web_routes)
        .layer(TraceLayer::new_for_http())
        .layer(Extension(state))
}
```

### 6.2 ä»»åŠ¡æäº¤ API

```rust
// crates/ailab-proxy/src/api/tasks.rs

use axum::{
    extract::{Extension, Path, Query},
    Json,
    http::StatusCode,
};
use std::sync::Arc;
use uuid::Uuid;

use crate::server::AppState;
use ailab_core::{
    types::{task::*, message::*},
    transport::Channel,
};

#[derive(serde::Deserialize)]
pub struct SubmitRequest {
    pub config: TaskConfig,
    pub project_dir: String,
    pub env_hash: String,
    pub project_hash: String,
    pub ignore_patterns: Vec<String>,
}

#[derive(serde::Serialize)]
pub struct SubmitResponse {
    pub task_id: String,
    pub message: String,
}

pub async fn submit(
    Extension(state): Extension<Arc<AppState>>,
    Json(request): Json<SubmitRequest>,
) -> Result<Json<SubmitResponse>, (StatusCode, String)> {
    // è·å–å½“å‰ç”¨æˆ·ï¼ˆä»è®¤è¯ä¸­é—´ä»¶ï¼‰
    let user_id = "current_user".to_string();  // TODO: ä»è®¤è¯è·å–
    
    let task_id = Uuid::new_v4();
    
    // 1. æ£€æŸ¥ç¼“å­˜ï¼Œç¡®å®šéœ€è¦ä¸Šä¼ çš„å†…å®¹
    let env_cached = state.cache_service.has_env(&request.env_hash).await
        .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, e.to_string()))?;
    let project_cached = state.cache_service.has_project(&request.project_hash).await
        .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, e.to_string()))?;
    
    // 2. ä¸Šä¼ æœªç¼“å­˜çš„å†…å®¹
    let project_path = std::path::Path::new(&request.project_dir);
    
    let storage_keys = StorageKeys {
        env: format!("envs/{}.tar.gz", request.env_hash),
        project: format!("projects/{}.tar.gz", request.project_hash),
        data: vec![],  // TODO: å¤„ç†æ•°æ®æ–‡ä»¶
        whls: vec![],  // TODO: å¤„ç† wheel æ–‡ä»¶
    };
    
    if !env_cached {
        state.sync_service.upload_env(
            project_path,
            &storage_keys.env,
        ).await.map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, e.to_string()))?;
        
        state.cache_service.mark_env_cached(&request.env_hash, &storage_keys.env).await
            .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, e.to_string()))?;
    }
    
    if !project_cached {
        state.sync_service.upload_project(
            project_path,
            &storage_keys.project,
            &request.ignore_patterns,
        ).await.map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, e.to_string()))?;
        
        state.cache_service.mark_project_cached(&request.project_hash, &storage_keys.project).await
            .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, e.to_string()))?;
    }
    
    // 3. åˆ›å»ºä»»åŠ¡è®°å½•
    let task = Task {
        id: task_id,
        user_id: user_id.clone(),
        config: request.config,
        status: TaskStatus::Pending,
        env_hash: request.env_hash,
        project_hash: request.project_hash,
        data_hashes: vec![],
        whl_hashes: vec![],
        created_at: chrono::Utc::now(),
        updated_at: chrono::Utc::now(),
        started_at: None,
        completed_at: None,
        slurm_job_id: None,
        exit_code: None,
        error_message: None,
        gpu_seconds: None,
        cpu_seconds: None,
    };
    
    state.db.insert_task(&task).await
        .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, e.to_string()))?;
    
    // 4. å‘é€æ¶ˆæ¯åˆ°è¿œç«¯
    let message = ControlMessage::new(MessagePayload::SubmitTask {
        task: task.clone(),
        storage_keys,
    });
    
    state.transport.send(Channel::ToRemote, message).await
        .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, e.to_string()))?;
    
    Ok(Json(SubmitResponse {
        task_id: task_id.to_string(),
        message: if env_cached && project_cached {
            "Task submitted (using cached environment and project)".to_string()
        } else {
            "Task submitted (files uploaded)".to_string()
        },
    }))
}

#[derive(serde::Deserialize)]
pub struct ListQuery {
    pub limit: Option<usize>,
    pub user_id: Option<String>,
}

pub async fn list(
    Extension(state): Extension<Arc<AppState>>,
    Query(query): Query<ListQuery>,
) -> Result<Json<Vec<Task>>, (StatusCode, String)> {
    let user_id = "current_user".to_string();  // TODO: ä»è®¤è¯è·å–
    let limit = query.limit.unwrap_or(20);
    
    let tasks = state.db.list_tasks(&user_id, limit).await
        .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, e.to_string()))?;
    
    Ok(Json(tasks))
}

pub async fn get(
    Extension(state): Extension<Arc<AppState>>,
    Path(id): Path<String>,
) -> Result<Json<Task>, (StatusCode, String)> {
    let task_id = Uuid::parse_str(&id)
        .map_err(|_| (StatusCode::BAD_REQUEST, "Invalid task ID".to_string()))?;
    
    let task = state.db.get_task(&task_id).await
        .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, e.to_string()))?
        .ok_or((StatusCode::NOT_FOUND, "Task not found".to_string()))?;
    
    Ok(Json(task))
}

pub async fn cancel(
    Extension(state): Extension<Arc<AppState>>,
    Path(id): Path<String>,
) -> Result<Json<serde_json::Value>, (StatusCode, String)> {
    let task_id = Uuid::parse_str(&id)
        .map_err(|_| (StatusCode::BAD_REQUEST, "Invalid task ID".to_string()))?;
    let user_id = "current_user".to_string();
    
    // å‘é€å–æ¶ˆæ¶ˆæ¯
    let message = ControlMessage::new(MessagePayload::CancelTask {
        task_id,
        user_id,
    });
    
    state.transport.send(Channel::ToRemote, message).await
        .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, e.to_string()))?;
    
    Ok(Json(serde_json::json!({
        "message": "Cancel request sent"
    })))
}
```

### 6.3 æ¶ˆæ¯è½®è¯¢æœåŠ¡

```rust
// crates/ailab-proxy/src/services/poller.rs

use std::sync::Arc;
use std::time::Duration;
use tokio::sync::broadcast;
use tracing::{info, warn, error};

use ailab_core::{
    transport::{Transport, Channel},
    types::message::{ControlMessage, MessagePayload},
};
use crate::db::Database;

pub struct PollerService {
    transport: Arc<dyn Transport>,
    db: Database,
    shutdown_rx: broadcast::Receiver<()>,
}

impl PollerService {
    pub fn new(
        transport: Arc<dyn Transport>,
        db: Database,
        shutdown_rx: broadcast::Receiver<()>,
    ) -> Self {
        Self {
            transport,
            db,
            shutdown_rx,
        }
    }
    
    pub async fn run(mut self) {
        info!("Starting message poller");
        
        loop {
            tokio::select! {
                _ = self.shutdown_rx.recv() => {
                    info!("Poller shutting down");
                    break;
                }
                result = self.poll_once() => {
                    if let Err(e) = result {
                        error!("Poll error: {}", e);
                        tokio::time::sleep(Duration::from_secs(5)).await;
                    }
                }
            }
        }
    }
    
    async fn poll_once(&self) -> anyhow::Result<()> {
        let message = self.transport
            .receive(Channel::ToLocal, Duration::from_secs(10))
            .await?;
        
        if let Some(msg) = message {
            self.handle_message(&msg).await?;
            self.transport.ack(Channel::ToLocal, &msg.id).await?;
        }
        
        Ok(())
    }
    
    async fn handle_message(&self, message: &ControlMessage) -> anyhow::Result<()> {
        match &message.payload {
            MessagePayload::StatusUpdate {
                task_id,
                status,
                slurm_job_id,
                progress,
                error_message,
            } => {
                info!("Task {} status: {:?}", task_id, status);
                
                self.db.update_task_status(
                    task_id,
                    *status,
                    slurm_job_id.clone(),
                    error_message.clone(),
                ).await?;
            }
            
            MessagePayload::TaskCompleted {
                task_id,
                status,
                exit_code,
                result_keys,
                gpu_seconds,
                cpu_seconds,
                error_message,
            } => {
                info!("Task {} completed: {:?}", task_id, status);
                
                self.db.complete_task(
                    task_id,
                    *status,
                    *exit_code,
                    *gpu_seconds,
                    *cpu_seconds,
                    result_keys.clone(),
                    error_message.clone(),
                ).await?;
            }
            
            MessagePayload::Heartbeat { source, timestamp } => {
                info!("Heartbeat from {} at {}", source, timestamp);
            }
            
            _ => {
                warn!("Unexpected message type: {:?}", message.payload);
            }
        }
        
        Ok(())
    }
}
```

## 7. Remote Server (ailab-remote)

### 7.1 ä¸»æœåŠ¡å™¨å¾ªç¯

```rust
// crates/ailab-remote/src/server.rs

use std::sync::Arc;
use std::time::Duration;
use tokio::sync::broadcast;
use tracing::{info, warn, error};

use ailab_core::{
    transport::{Transport, Channel},
    storage::Storage,
    types::message::{ControlMessage, MessagePayload},
};
use crate::{
    services::{TaskProcessor, SlurmMonitor, Accounting},
    db::Database,
};

pub struct RemoteServer {
    transport: Arc<dyn Transport>,
    storage: Arc<dyn Storage>,
    db: Database,
    processor: TaskProcessor,
    monitor: SlurmMonitor,
    accounting: Accounting,
}

impl RemoteServer {
    pub async fn new(
        transport: Arc<dyn Transport>,
        storage: Arc<dyn Storage>,
        db: Database,
        config: &crate::Config,
    ) -> anyhow::Result<Self> {
        let processor = TaskProcessor::new(
            storage.clone(),
            config.paths.clone(),
        );
        let monitor = SlurmMonitor::new();
        let accounting = Accounting::new(db.clone());
        
        Ok(Self {
            transport,
            storage,
            db,
            processor,
            monitor,
            accounting,
        })
    }
    
    pub async fn run(self, mut shutdown_rx: broadcast::Receiver<()>) {
        info!("Remote server starting");
        
        // å¯åŠ¨ Slurm ç›‘æ§ä»»åŠ¡
        let monitor = self.monitor.clone();
        let transport = self.transport.clone();
        let db = self.db.clone();
        let accounting = self.accounting.clone();
        
        let monitor_handle = tokio::spawn(async move {
            Self::run_monitor(monitor, transport, db, accounting).await;
        });
        
        // ä¸»æ¶ˆæ¯å¤„ç†å¾ªç¯
        loop {
            tokio::select! {
                _ = shutdown_rx.recv() => {
                    info!("Server shutting down");
                    break;
                }
                result = self.poll_and_process() => {
                    if let Err(e) = result {
                        error!("Processing error: {}", e);
                        tokio::time::sleep(Duration::from_secs(5)).await;
                    }
                }
            }
        }
        
        monitor_handle.abort();
    }
    
    async fn poll_and_process(&self) -> anyhow::Result<()> {
        let message = self.transport
            .receive(Channel::ToRemote, Duration::from_secs(5))
            .await?;
        
        if let Some(msg) = message {
            self.handle_message(&msg).await?;
            self.transport.ack(Channel::ToRemote, &msg.id).await?;
        }
        
        Ok(())
    }
    
    async fn handle_message(&self, message: &ControlMessage) -> anyhow::Result<()> {
        match &message.payload {
            MessagePayload::SubmitTask { task, storage_keys } => {
                info!("Processing task submission: {}", task.id);
                
                // æ›´æ–°çŠ¶æ€ä¸º Preparing
                self.send_status_update(&task.id, TaskStatus::Preparing, None, None).await?;
                
                // å¤„ç†ä»»åŠ¡
                match self.processor.process(task, storage_keys).await {
                    Ok(slurm_job_id) => {
                        // è®°å½•ä»»åŠ¡
                        self.db.insert_task(task, &slurm_job_id).await?;
                        
                        // æ›´æ–°çŠ¶æ€ä¸º Queued
                        self.send_status_update(
                            &task.id,
                            TaskStatus::Queued,
                            Some(slurm_job_id),
                            None,
                        ).await?;
                    }
                    Err(e) => {
                        error!("Task processing failed: {}", e);
                        self.send_status_update(
                            &task.id,
                            TaskStatus::Failed,
                            None,
                            Some(e.to_string()),
                        ).await?;
                    }
                }
            }
            
            MessagePayload::CancelTask { task_id, user_id } => {
                info!("Cancelling task: {}", task_id);
                
                if let Some(job_id) = self.db.get_slurm_job_id(task_id).await? {
                    self.monitor.cancel_job(&job_id).await?;
                }
            }
            
            MessagePayload::QueryStatus { task_ids } => {
                let statuses = self.db.get_task_statuses(task_ids).await?;
                
                let response = ControlMessage::new(MessagePayload::StatusResponse {
                    tasks: statuses,
                });
                
                self.transport.send(Channel::ToLocal, response).await?;
            }
            
            _ => {
                warn!("Unexpected message type");
            }
        }
        
        Ok(())
    }
    
    async fn send_status_update(
        &self,
        task_id: &TaskId,
        status: TaskStatus,
        slurm_job_id: Option<String>,
        error_message: Option<String>,
    ) -> anyhow::Result<()> {
        let message = ControlMessage::new(MessagePayload::StatusUpdate {
            task_id: *task_id,
            status,
            slurm_job_id,
            progress: None,
            error_message,
        });
        
        self.transport.send(Channel::ToLocal, message).await?;
        Ok(())
    }
    
    async fn run_monitor(
        monitor: SlurmMonitor,
        transport: Arc<dyn Transport>,
        db: Database,
        accounting: Accounting,
    ) {
        let mut interval = tokio::time::interval(Duration::from_secs(30));
        
        loop {
            interval.tick().await;
            
            // è·å–æ‰€æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡
            let running_tasks = match db.get_running_tasks().await {
                Ok(tasks) => tasks,
                Err(e) => {
                    error!("Failed to get running tasks: {}", e);
                    continue;
                }
            };
            
            for (task_id, slurm_job_id) in running_tasks {
                match monitor.check_job_status(&slurm_job_id).await {
                    Ok(status) => {
                        if status.is_terminal() {
                            // ä»»åŠ¡ç»“æŸï¼Œå¤„ç†ç»“æœ
                            // ... ä¸Šä¼ ç»“æœã€æ›´æ–°ç»Ÿè®¡ç­‰
                        }
                    }
                    Err(e) => {
                        error!("Failed to check job {}: {}", slurm_job_id, e);
                    }
                }
            }
        }
    }
}
```

### 7.2 ä»»åŠ¡å¤„ç†å™¨

```rust
// crates/ailab-remote/src/services/processor.rs

use std::path::{Path, PathBuf};
use std::sync::Arc;
use tokio::process::Command;
use tracing::info;

use ailab_core::{
    storage::Storage,
    types::{task::Task, message::StorageKeys},
};

#[derive(Clone)]
pub struct PathConfig {
    pub envs_dir: PathBuf,
    pub tasks_dir: PathBuf,
    pub cache_dir: PathBuf,
}

pub struct TaskProcessor {
    storage: Arc<dyn Storage>,
    paths: PathConfig,
}

impl TaskProcessor {
    pub fn new(storage: Arc<dyn Storage>, paths: PathConfig) -> Self {
        Self { storage, paths }
    }
    
    pub async fn process(
        &self,
        task: &Task,
        storage_keys: &StorageKeys,
    ) -> anyhow::Result<String> {
        let task_dir = self.paths.tasks_dir.join(task.id.to_string());
        std::fs::create_dir_all(&task_dir)?;
        
        // 1. å‡†å¤‡ç¯å¢ƒ
        let env_dir = self.ensure_environment(&task.env_hash, &storage_keys.env).await?;
        
        // 2. ä¸‹è½½é¡¹ç›®
        let project_dir = task_dir.join("project");
        self.storage.download_dir(&storage_keys.project, &project_dir).await?;
        
        // 3. ä¸‹è½½æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        for data_key in &storage_keys.data {
            let data_name = data_key.split('/').last().unwrap_or("data");
            let data_dir = task_dir.join("data").join(data_name);
            self.storage.download_dir(data_key, &data_dir).await?;
        }
        
        // 4. å®‰è£…é¢å¤–çš„ wheels
        for whl_key in &storage_keys.whls {
            let whl_path = task_dir.join("whls").join(
                whl_key.split('/').last().unwrap_or("package.whl")
            );
            self.storage.download(whl_key, &whl_path).await?;
            
            self.install_wheel(&env_dir, &whl_path).await?;
        }
        
        // 5. ç”Ÿæˆ Slurm è„šæœ¬
        let script_path = self.generate_slurm_script(task, &task_dir, &env_dir, &project_dir)?;
        
        // 6. æäº¤åˆ° Slurm
        let job_id = self.submit_to_slurm(&script_path).await?;
        
        Ok(job_id)
    }
    
    async fn ensure_environment(
        &self,
        env_hash: &str,
        storage_key: &str,
    ) -> anyhow::Result<PathBuf> {
        let env_dir = self.paths.envs_dir.join(env_hash);
        
        if env_dir.exists() {
            info!("Using cached environment: {}", env_hash);
            return Ok(env_dir);
        }
        
        info!("Building environment: {}", env_hash);
        
        // ä¸‹è½½ç¯å¢ƒæ–‡ä»¶
        let env_files_dir = self.paths.cache_dir.join("env_files").join(env_hash);
        self.storage.download_dir(storage_key, &env_files_dir).await?;
        
        // åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
        let venv_dir = env_dir.join(".venv");
        Command::new("uv")
            .args(["venv", venv_dir.to_str().unwrap()])
            .current_dir(&env_files_dir)
            .status()
            .await?;
        
        // å®‰è£…ä¾èµ–
        Command::new("uv")
            .args(["sync", "--frozen"])
            .current_dir(&env_files_dir)
            .env("VIRTUAL_ENV", &venv_dir)
            .status()
            .await?;
        
        Ok(env_dir)
    }
    
    async fn install_wheel(&self, env_dir: &Path, whl_path: &Path) -> anyhow::Result<()> {
        let venv_dir = env_dir.join(".venv");
        
        Command::new("uv")
            .args(["pip", "install", whl_path.to_str().unwrap()])
            .env("VIRTUAL_ENV", &venv_dir)
            .status()
            .await?;
        
        Ok(())
    }
    
    fn generate_slurm_script(
        &self,
        task: &Task,
        task_dir: &Path,
        env_dir: &Path,
        project_dir: &Path,
    ) -> anyhow::Result<PathBuf> {
        let script_path = task_dir.join("job.sh");
        let venv_activate = env_dir.join(".venv/bin/activate");
        
        let workdir = if task.config.run.workdir == "." {
            project_dir.to_path_buf()
        } else {
            project_dir.join(&task.config.run.workdir)
        };
        
        let command = task.config.run.command.clone()
            .or_else(|| task.config.run.script.as_ref().map(|s| format!("bash {}", s)))
            .unwrap_or_else(|| "echo 'No command specified'".to_string());
        
        let script = format!(r#"#!/bin/bash
#SBATCH --job-name=ailab-{task_id}
#SBATCH --gres=gpu:{gpus}
#SBATCH --cpus-per-task={cpus}
#SBATCH --mem={memory}
#SBATCH --time={time_limit}
#SBATCH --output={task_dir}/slurm_%j.out
#SBATCH --error={task_dir}/slurm_%j.err

set -e

# Activate environment
source {venv_activate}

# Enter working directory
cd {workdir}

# Run command
{command}

# Save exit code
echo $? > {task_dir}/exit_code
"#,
            task_id = task.id,
            gpus = task.config.resources.gpus,
            cpus = task.config.resources.cpus,
            memory = task.config.resources.memory,
            time_limit = task.config.resources.time_limit,
            task_dir = task_dir.display(),
            venv_activate = venv_activate.display(),
            workdir = workdir.display(),
            command = command,
        );
        
        std::fs::write(&script_path, script)?;
        
        Ok(script_path)
    }
    
    async fn submit_to_slurm(&self, script_path: &Path) -> anyhow::Result<String> {
        let output = Command::new("sbatch")
            .arg(script_path)
            .output()
            .await?;
        
        if !output.status.success() {
            anyhow::bail!(
                "sbatch failed: {}",
                String::from_utf8_lossy(&output.stderr)
            );
        }
        
        // è§£æè¾“å‡ºè·å– job ID
        // æ ¼å¼: "Submitted batch job 12345"
        let stdout = String::from_utf8_lossy(&output.stdout);
        let job_id = stdout
            .split_whitespace()
            .last()
            .ok_or_else(|| anyhow::anyhow!("Failed to parse job ID"))?
            .to_string();
        
        Ok(job_id)
    }
}
```

## 8. é…ç½®æ–‡ä»¶

### 8.1 Proxy é…ç½®

```toml
# config/proxy.toml

[server]
host = "0.0.0.0"
port = 8800

# æ§åˆ¶é¢ä¼ è¾“é…ç½®
[transport]
type = "s3"
endpoint = "http://minio.internal:9000"
bucket = "ailab-course"
access_key = "minioadmin"
secret_key = "minioadmin"
prefix = "messages"
poll_interval_ms = 5000

# æˆ–è€…ä½¿ç”¨ Redis
# [transport]
# type = "redis"
# url = "redis://redis.internal:6379"
# prefix = "ailab"

# æ•°æ®é¢å­˜å‚¨é…ç½®
[storage]
type = "s3"
endpoint = "http://minio.internal:9000"
bucket = "ailab-course"
access_key = "minioadmin"
secret_key = "minioadmin"

[database]
path = "/var/lib/ailab-proxy/data.db"

[cache]
max_size_gb = 100
cleanup_threshold_percent = 90
```

### 8.2 Remote é…ç½®

```toml
# config/remote.toml

[transport]
type = "s3"
endpoint = "http://minio.internal:9000"
bucket = "ailab-course"
access_key = "minioadmin"
secret_key = "minioadmin"
prefix = "messages"
poll_interval_ms = 3000

[storage]
type = "s3"
endpoint = "http://minio.internal:9000"
bucket = "ailab-course"
access_key = "minioadmin"
secret_key = "minioadmin"

[paths]
envs_dir = "/data/ailab/envs"
tasks_dir = "/data/ailab/tasks"
cache_dir = "/data/ailab/cache"

[slurm]
partition = "gpu"
default_qos = "normal"

[database]
path = "/data/ailab/accounting.db"
```

## 9. å­˜å‚¨ç»“æ„

```
bucket: ailab-course/
â”œâ”€â”€ messages/                    # æ§åˆ¶é¢æ¶ˆæ¯
â”‚   â”œâ”€â”€ to_remote/              # Local â†’ Remote
â”‚   â”‚   â””â”€â”€ {msg_id}.json
â”‚   â”œâ”€â”€ to_local/               # Remote â†’ Local
â”‚   â”‚   â””â”€â”€ {msg_id}.json
â”‚   â””â”€â”€ processed/              # å·²å¤„ç†çš„æ¶ˆæ¯å½’æ¡£
â”‚       â””â”€â”€ {date}/{msg_id}.json
â”‚
â”œâ”€â”€ envs/                        # ç¯å¢ƒæ–‡ä»¶
â”‚   â””â”€â”€ {env_hash}.tar.gz       # åŒ…å« uv.lock, pyproject.toml
â”‚
â”œâ”€â”€ projects/                    # é¡¹ç›®ä»£ç 
â”‚   â””â”€â”€ {project_hash}.tar.gz
â”‚
â”œâ”€â”€ datasets/                    # æ•°æ®é›†
â”‚   â””â”€â”€ {dataset_hash}/
â”‚
â”œâ”€â”€ whls/                        # è‡ªå®šä¹‰ wheel
â”‚   â””â”€â”€ {whl_hash}.whl
â”‚
â””â”€â”€ results/                     # ä»»åŠ¡ç»“æœ
    â””â”€â”€ {task_id}/
        â”œâ”€â”€ outputs/
        â”œâ”€â”€ logs/
        â””â”€â”€ metadata.json
```

## 10. å®ç°è·¯çº¿å›¾

### Phase 1: æ ¸å¿ƒåŠŸèƒ½ (3-4 å¤©)

1. **ailab-core**: ç±»å‹å®šä¹‰ã€trait å®šä¹‰ã€hash å·¥å…·
2. **ailab-transport**: S3 Transport å®ç°
3. **ailab-storage**: S3 Storage å®ç°
4. **ailab-cli**: åŸºç¡€å‘½ä»¤ (submit, status, list)
5. **ailab-proxy**: HTTP APIã€åŸºç¡€ä»»åŠ¡ç®¡ç†ã€æ¶ˆæ¯è½®è¯¢
6. **ailab-remote**: æ¶ˆæ¯å¤„ç†ã€ç¯å¢ƒæ„å»ºã€Slurm æäº¤

### Phase 2: å®Œå–„åŠŸèƒ½ (2-3 å¤©)

1. **ailab-cli**: logs, cancel, pull å‘½ä»¤
2. **ailab-proxy**: ç”¨æˆ·è®¤è¯ã€ç¼“å­˜ç®¡ç†
3. **ailab-remote**: ä»»åŠ¡ç›‘æ§ã€ç»“æœä¸Šä¼ ã€ç»Ÿè®¡
4. **ailab-transport**: Redis Transport å®ç°

### Phase 3: Web UI å’Œä¼˜åŒ– (2 å¤©)

1. **ailab-proxy**: Web çŠ¶æ€é¡µé¢
2. é”™è¯¯å¤„ç†å®Œå–„
3. é‡è¯•æœºåˆ¶
4. æ–‡æ¡£

## 11. å…³é”®è®¾è®¡å†³ç­–æ€»ç»“

| å†³ç­–ç‚¹ | é€‰æ‹© | ç†ç”± |
|--------|------|------|
| Transport æŠ½è±¡ | Trait + å¤šå®ç° | çµæ´»æ”¯æŒä¸åŒåç«¯ï¼Œä¾¿äºæµ‹è¯• |
| Control/Data åˆ†ç¦» | æ¶ˆæ¯ç”¨ Transportï¼Œæ–‡ä»¶ç”¨ Storage | å¤§æ–‡ä»¶å§‹ç»ˆèµ°å¯¹è±¡å­˜å‚¨ï¼Œæ¶ˆæ¯å¯çµæ´»é€‰æ‹© |
| æ¶ˆæ¯æ ¼å¼ | JSON + ç±»å‹æ ‡ç­¾ | å¯è¯»æ€§å¥½ï¼Œè°ƒè¯•æ–¹ä¾¿ |
| ç¯å¢ƒç®¡ç† | uv + åŸºäº hash çš„ç¼“å­˜ | ç®€å•å¯é ï¼Œé¿å…é‡å¤æ„å»º |
| æ•°æ®åº“ | SQLite | è½»é‡ï¼Œå•æœºè¶³å¤Ÿï¼Œæ— éœ€é¢å¤–éƒ¨ç½² |
| Web æ¡†æ¶ | Axum | ç°ä»£ã€ç±»å‹å®‰å…¨ã€æ€§èƒ½å¥½ |
| CLI æ¡†æ¶ | Clap derive | ç±»å‹å®‰å…¨ï¼Œè‡ªåŠ¨ç”Ÿæˆå¸®åŠ© |

